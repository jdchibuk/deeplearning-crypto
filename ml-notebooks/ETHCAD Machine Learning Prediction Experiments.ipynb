{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuk/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; } .CodeMirror pre {font-size: 9pt;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using keras and sklearn for random forest and neural network\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "from scipy.signal import *\n",
    "from scipy.ndimage.filters import gaussian_filter1d as gfilter\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import lag_plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import sqlite3\n",
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "import pylab\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; } .CodeMirror pre {font-size: 9pt;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the sqlite table and name it from your raw data csv collected from your exchange\n",
    "\n",
    "table_name = \"ETHCAD\"\n",
    "conn = sqlite3.connect('pair_history_ethcad.db')\n",
    "\n",
    "df = pd.read_csv('20180124ethcad0730.csv')\n",
    "df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the table to a dataframe\n",
    "\n",
    "conn = sqlite3.connect('pair_history_ethcad.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "TARGET = table_name\n",
    "ETHCAD = pd.read_sql_query('SELECT * FROM ' + TARGET , conn)\n",
    "\n",
    "#print(ETHCAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'w', u'h', u'm', u'ask', u'bid', u'spread', u'volume'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ETHCAD.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuk/anaconda2/lib/python2.7/site-packages/seaborn/categorical.py:1460: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
      "  stat_data = remove_na(group_data)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>m</th>\n",
       "      <th>ask</th>\n",
       "      <th>bid</th>\n",
       "      <th>spread</th>\n",
       "      <th>volume</th>\n",
       "      <th>diffask</th>\n",
       "      <th>diffbid</th>\n",
       "      <th>diffvolume</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>16.20</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-1.304900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003151</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.415027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>-1.873849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-8.97</td>\n",
       "      <td>-35.017690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>24.16</td>\n",
       "      <td>0.010621</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>16.030270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>23.97</td>\n",
       "      <td>-0.004811</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7.038194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-0.002122</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.263706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>23.17</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.664624</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>21.99</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.01</td>\n",
       "      <td>-0.230758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003165</td>\n",
       "      <td>17.98</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.090941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>14.98</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.873114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>13.98</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>1.27</td>\n",
       "      <td>9.00</td>\n",
       "      <td>7.456382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.001020</td>\n",
       "      <td>-0.007149</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-0.002260</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.462134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-1.365271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.160538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.000795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.26</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.545904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.003987</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>6.75</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.774811</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.005427</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.716623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.001609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.557935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.162308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.933116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-17.99</td>\n",
       "      <td>7.426112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>23.98</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-10.711265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>55</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>20.75</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>40.249222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>20.69</td>\n",
       "      <td>-0.012250</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.017524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>20.78</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.777833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>17.98</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>59</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.001582</td>\n",
       "      <td>15.97</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.067082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>12.96</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.125845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.00</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.635039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.96</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.217913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.055359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.001844</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.074569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-10.902280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.087061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.039845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.424599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.093901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.73</td>\n",
       "      <td>-0.000496</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-21.481592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.009737</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.290895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3.459491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.715556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.826100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.338983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.317460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.870063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.859315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.551924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.055614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.665110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1552 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      w   h   m       ask       bid  spread    volume  diffask  diffbid  \\\n",
       "1     2  23  32  0.000008 -0.000016   16.20  0.029360     0.00     3.97   \n",
       "2     2  23  33  0.000000 -0.003151   12.23  0.000396     0.00     0.00   \n",
       "3     2  23  34  0.000000  0.000000   12.23  0.000430    -0.23    -4.00   \n",
       "4     2  23  35  0.000185  0.003185   16.00  0.000569    -0.81    -8.97   \n",
       "5     2  23  36  0.000651  0.007119   24.16  0.010621    -0.01     0.18   \n",
       "6     2  23  37  0.000008 -0.000142   23.97 -0.004811     0.02     0.09   \n",
       "7     2  23  38 -0.000016 -0.000071   23.90 -0.002122    -0.03     0.70   \n",
       "8     2  23  39  0.000024 -0.000552   23.17 -0.000382    -0.18     1.00   \n",
       "9     2  23  40  0.000145 -0.000789   21.99 -0.000503     0.00     4.01   \n",
       "10    2  23  41  0.000000 -0.003165   17.98  0.000070     0.00     3.00   \n",
       "11    2  23  42  0.000000 -0.002375   14.98 -0.000935    -0.01     0.99   \n",
       "12    2  23  43  0.000008 -0.000786   13.98 -0.001172     1.27     9.00   \n",
       "13    2  23  44 -0.001020 -0.007149    6.25 -0.002260    -0.25     1.02   \n",
       "14    2  23  45  0.000201 -0.000816    4.98 -0.001659    -1.00    -0.01   \n",
       "15    2  23  46  0.000804  0.000008    3.99  0.000415     0.99     0.00   \n",
       "16    2  23  47 -0.000795  0.000000    4.98  0.000049     0.26     4.98   \n",
       "17    2  23  48 -0.000209 -0.003987    0.26 -0.000774     6.75     4.01   \n",
       "18    2  23  49 -0.005427 -0.003223    3.00 -0.000236     1.99     0.00   \n",
       "19    2  23  50 -0.001609  0.000000    4.99 -0.000523     1.01     0.00   \n",
       "20    2  23  51 -0.000818  0.000000    6.00  0.001388     0.00     0.00   \n",
       "21    2  23  52  0.000000  0.000000    6.00 -0.000049     0.00     0.00   \n",
       "22    2  23  53  0.000000  0.000000    6.00 -0.001501    -0.01   -17.99   \n",
       "23    2  23  54  0.000008  0.014508   23.98 -0.002262    -3.99    -0.76   \n",
       "24    2  23  55  0.003233  0.000604   20.75  0.003271     0.00     0.06   \n",
       "25    2  23  56  0.000000 -0.000048   20.69 -0.012250     0.00    -0.09   \n",
       "26    2  23  57  0.000000  0.000072   20.78 -0.000005    -2.00     0.80   \n",
       "27    2  23  58  0.001616 -0.000636   17.98 -0.000240    -0.02     1.99   \n",
       "28    2  23  59  0.000016 -0.001582   15.97 -0.000003    -0.02     2.99   \n",
       "29    3   0   0  0.000016 -0.002381   12.96 -0.000329    -0.96     0.00   \n",
       "30    3   0   1  0.000774  0.000000   12.00 -0.000347     0.00     0.00   \n",
       "...  ..  ..  ..       ...       ...     ...       ...      ...      ...   \n",
       "1523  3  12   6  0.000008  0.000000    4.96 -0.002001     0.02     0.00   \n",
       "1524  3  12   7 -0.000016  0.000000    4.98  0.000000     0.00     0.00   \n",
       "1525  3  12   8  0.000000  0.000000    4.98  0.003754     0.00     0.00   \n",
       "1526  3  12   9  0.000000  0.000000    4.98  0.000910     0.00     0.00   \n",
       "1527  3  12  10  0.000000  0.000000    4.98 -0.000091     0.00     0.00   \n",
       "1528  3  12  11  0.000000  0.000000    4.98  0.000000     0.00     0.00   \n",
       "1529  3  12  12  0.000000  0.000000    4.98  0.000000     0.00     0.00   \n",
       "1530  3  12  13  0.000000  0.000000    4.98  0.000000     0.00     0.00   \n",
       "1531  3  12  14  0.000000  0.000000    4.98  0.000000     0.00     0.00   \n",
       "1532  3  12  15  0.000000  0.000000    4.98 -0.000136     0.00     0.00   \n",
       "1533  3  12  16  0.000000  0.000000    4.98 -0.001844     0.00     0.00   \n",
       "1534  3  12  17  0.000000  0.000000    4.98 -0.000034     0.00     0.00   \n",
       "1535  3  12  18  0.000000  0.000000    4.98  0.004967     0.00     0.00   \n",
       "1536  3  12  19  0.000000  0.000000    4.98  0.001853     0.00     0.00   \n",
       "1537  3  12  20  0.000000  0.000000    4.98 -0.000018     0.00     0.00   \n",
       "1538  3  12  21  0.000000  0.000000    4.98 -0.001097    -0.25     0.00   \n",
       "1539  3  12  22  0.000199  0.000000    4.73 -0.000496     0.25     0.00   \n",
       "1540  3  12  23 -0.000199  0.000000    4.98  0.009737     0.00     0.00   \n",
       "1541  3  12  24  0.000000  0.000000    4.98  0.000131    -0.98     0.00   \n",
       "1542  3  12  25  0.000781  0.000000    4.00 -0.000223     0.00     0.00   \n",
       "1543  3  12  26  0.000000  0.000000    4.00  0.001553     0.00     0.00   \n",
       "1544  3  12  27  0.000000  0.000000    4.00 -0.000769     0.00     0.00   \n",
       "1545  3  12  28  0.000000  0.000000    4.00 -0.000819     0.00     0.00   \n",
       "1546  3  12  29  0.000000  0.000000    4.00 -0.004642     0.00     0.00   \n",
       "1547  3  12  30  0.000000  0.000000    4.00  0.000143     0.00     0.00   \n",
       "1548  3  12  31  0.000000  0.000000    4.00  0.000000     0.00     0.00   \n",
       "1549  3  12  32  0.000000  0.000000    4.00  0.000392     0.00     0.00   \n",
       "1550  3  12  33  0.000000  0.000000    4.00  0.000387     0.00     0.00   \n",
       "1551  3  12  34  0.000000  0.000000    4.00  0.000699     0.00     0.00   \n",
       "1552  3  12  35  0.000000  0.000000    4.00 -0.004528     0.00     0.00   \n",
       "\n",
       "      diffvolume class  \n",
       "1      -1.304900     1  \n",
       "2      -1.415027     1  \n",
       "3      -1.873849     1  \n",
       "4     -35.017690     1  \n",
       "5      16.030270     1  \n",
       "6       7.038194     0  \n",
       "7       1.263706     1  \n",
       "8       1.664624     1  \n",
       "9      -0.230758     1  \n",
       "10      3.090941     1  \n",
       "11      3.873114     1  \n",
       "12      7.456382     0  \n",
       "13      5.462134     1  \n",
       "14     -1.365271     1  \n",
       "15     -0.160538     0  \n",
       "16      2.545904     0  \n",
       "17      0.774811     0  \n",
       "18      1.716623     0  \n",
       "19     -4.557935     0  \n",
       "20      0.162308     1  \n",
       "21      4.933116     1  \n",
       "22      7.426112     1  \n",
       "23    -10.711265     1  \n",
       "24     40.249222     1  \n",
       "25      0.017524     1  \n",
       "26      0.777833     1  \n",
       "27      0.009976     1  \n",
       "28      1.067082     1  \n",
       "29      1.125845     1  \n",
       "30      4.635039     1  \n",
       "...          ...   ...  \n",
       "1523    0.000000     0  \n",
       "1524   -8.217913     1  \n",
       "1525   -2.000000     1  \n",
       "1526    0.200000     1  \n",
       "1527    0.000000     1  \n",
       "1528    0.000000     1  \n",
       "1529    0.000000     1  \n",
       "1530    0.000000     1  \n",
       "1531    0.300000     1  \n",
       "1532    4.055359     1  \n",
       "1533    0.074569     1  \n",
       "1534  -10.902280     1  \n",
       "1535   -4.087061     1  \n",
       "1536    0.039845     1  \n",
       "1537    2.424599     1  \n",
       "1538    1.093901     1  \n",
       "1539  -21.481592     0  \n",
       "1540   -0.290895     1  \n",
       "1541    0.497249     1  \n",
       "1542   -3.459491     1  \n",
       "1543    1.715556     1  \n",
       "1544    1.826100     1  \n",
       "1545   10.338983     1  \n",
       "1546   -0.317460     1  \n",
       "1547    0.000000     1  \n",
       "1548   -0.870063     1  \n",
       "1549   -0.859315     1  \n",
       "1550   -1.551924     1  \n",
       "1551   10.055614     1  \n",
       "1552   -0.665110     1  \n",
       "\n",
       "[1552 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c0f5aca50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ETHCAD['diffask'] = ETHCAD['ask'] - ETHCAD['ask'].shift(-1)\n",
    "ETHCAD['diffbid'] = ETHCAD['bid'] - ETHCAD['bid'].shift(-1)\n",
    "ETHCAD['diffvolume'] = ETHCAD['volume'] - ETHCAD['volume'].shift(-1)\n",
    "\n",
    "ETHCAD['bid'] = ETHCAD['bid'].pct_change()\n",
    "ETHCAD['ask'] = ETHCAD['ask'].pct_change()\n",
    "ETHCAD['volume'] = ETHCAD['volume'].pct_change()\n",
    "\n",
    "#0 = buy, 1 = pass\n",
    "def label_signal(diff):\n",
    "        if (diff > 0):\n",
    "            return '0'\n",
    "        elif (diff < 0):\n",
    "            return '1'\n",
    "        return '1'\n",
    "ETHCAD['class'] = ETHCAD['diffask'].map(lambda x: label_signal(x))\n",
    "ETHCAD.dropna(inplace=True)\n",
    "ETHCAD = ETHCAD[~(ETHCAD['class'] == 'neutral')]\n",
    "sns.countplot(x='class', data=ETHCAD)\n",
    "\n",
    "ETHCAD\n",
    "\n",
    "#TODO come up with a method to properly balance the dataframe between BUY and PASS classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1241, 7), (311, 7), (1552,), (311,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "X = ETHCAD.drop(['class','diffask','diffbid','diffvolume'], axis=1).values\n",
    "X = X.astype('float32')\n",
    "\n",
    "Y = ETHCAD['class'].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "trainX = X[:train_size]\n",
    "trainY = Y[:train_size]\n",
    "trainEncodedY = encoded_Y[:train_size]\n",
    "testX = X[train_size:]\n",
    "testY = Y[train_size:]\n",
    "testEncodedY = encoded_Y[train_size:]\n",
    "trainX.shape, testX.shape, encoded_Y.shape, testEncodedY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuk/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy :', 1.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAF3CAYAAADXQiMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c0f6fe290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#giving some unrealistic prediction results\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=2)\n",
    "clf = clf.fit(trainX, trainY)\n",
    "preds = clf.predict(testX)\n",
    "preds = preds.argmax(1)\n",
    "preds = encoder.inverse_transform(preds)\n",
    "confm = pd.crosstab(testEncodedY, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(confm, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\n",
    "print(\"accuracy :\", clf.score(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KERA neural net prediction model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense, merge, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import math\n",
    "import json\n",
    "import datetime\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the model paramaters based on inputs, layers and outputs\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = baseline_model()\n",
    "\n",
    "encodedY = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 138\n",
      "Trainable params: 138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model.summary()\n",
    "\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 992 samples, validate on 249 samples\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.24498, saving model to weights.best.hdf5\n",
      " - 0s - loss: 0.7331 - acc: 0.1885 - categorical_accuracy: 0.1885 - val_loss: 0.7176 - val_acc: 0.2450 - val_categorical_accuracy: 0.2450\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.24498 to 0.50201, saving model to weights.best.hdf5\n",
      " - 0s - loss: 0.7079 - acc: 0.3165 - categorical_accuracy: 0.3165 - val_loss: 0.6962 - val_acc: 0.5020 - val_categorical_accuracy: 0.5020\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.50201 to 0.70281, saving model to weights.best.hdf5\n",
      " - 0s - loss: 0.6877 - acc: 0.6331 - categorical_accuracy: 0.6331 - val_loss: 0.6789 - val_acc: 0.7028 - val_categorical_accuracy: 0.7028\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.70281 to 0.82329, saving model to weights.best.hdf5\n",
      " - 0s - loss: 0.6708 - acc: 0.8085 - categorical_accuracy: 0.8085 - val_loss: 0.6646 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6567 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6522 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6439 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6413 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6328 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6314 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6224 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6223 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6131 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6136 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.6039 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.6055 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5951 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5978 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5868 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5902 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5786 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5828 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5708 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5755 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5626 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5682 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5544 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5610 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5462 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5536 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5381 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5460 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5296 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5382 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve\n",
      " - 0s - loss: 0.5207 - acc: 0.8478 - categorical_accuracy: 0.8478 - val_loss: 0.5303 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c10a1e190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, encodedY[:train_size], epochs=20, batch_size=300, verbose=2, shuffle=True, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4.        ,  344.99999357, 2418.99999549,   43.58912428,\n",
       "          36.48610882,  132.14519523, 2420.00263837]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = [w,h,m,ask,bid,spread,volume,diffbid,diffvolume]\n",
    "X = [[2,15,41,1245.0,1240.01,4.990000000000009,4670.02846038]]\n",
    "X = scaler.inverse_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print prediction model\n",
    "\n",
    "model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save and load scaler for live trader\n",
    "joblib.dump(scaler, './' + TARGET + '_open_X_scaler_3.0.0.pkl') \n",
    "\n",
    "scaler    = joblib.load('./' + TARGET + '_open_X_scaler_3.0.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 138\n",
      "Trainable params: 138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#loading model and weights for live prediciton for experimenting notebook\n",
    "model = baseline_model()\n",
    "\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
